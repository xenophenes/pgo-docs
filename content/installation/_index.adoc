---
title: "Installation"
date: 2018-04-24T18:27:02-07:00
draft: true
---

:toc:
v2.6, {docdate}

{{%expand "Quick Installation" %}}

== Overview

There are currently *quickstart* scripts that seek to automate
the deployment to popular Kube environments -

 * link:https://github.com/CrunchyData/postgres-operator/blob/master/examples/quickstart-for-gke.sh[quickstart-for-gke.sh]
 * link:https://github.com/CrunchyData/postgres-operator/blob/master/examples/quickstart-for-ocp.sh[quickstart-for-ocp.sh]

The *quickstart-for-gke* script will deploy the operator to
a GKE Kube cluster.

The *quickstart-for-ocp* script will deploy the operator to
an Openshift Container Platform cluster.

Both scripts assume you have a StorageClass defined for persistence.

Pre-compiled versions of the Operator *pgo* client are provided for the x86_64, Mac OSX, and Windows hosts.

== Quickstart

=== GKE/PKS
The *quickstart-for-gke.sh* script will allow users to set up the Postgres Operator quickly on GKE including PKS.
This script is tested on GKE but can be modified for use with other Kubernetes environments as well.

The script requires a few things in order to work -

 * wget utility installed
 * kubectl utility installed
 * StorageClass defined

Executing the script will give you a default Operator deployment
that assumes *dynamic* storage and a storage class named *standard*,
things that GKE provides.

The script performs the following -

 * downloads the Operator configuration files
 * sets the $HOME/.pgouser file to default settings
 * deploys the Operator Deployment
 * sets your .bashrc to include the Operator environment variables
 * sets your $HOME/.bash_completion file to be the *pgo* bash_completion file

=== Openshift Container Platform

A similar script for installing the operator on OCP is
offered with similar features as the GKE script.  This script is
tested on OCP 3.7 with a StorageClass defined.

{{% /expand%}}

{{%expand "Manual Installation" %}}

== Project Structure

To perform an installation of the operator, first create the project structure as follows on your host, here we assume a local directory called *odev* -
....
export GOPATH=$HOME/odev
mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg
mkdir -p $GOPATH/src/github.com/crunchydata/
....

Next, get a tagged release of the source code -
....
cd $GOPATH/src/github.com/crunchydata
git clone https://github.com/CrunchyData/postgres-operator.git
cd postgres-operator
git checkout 2.6
....

== Installation Prerequsites

To run the operator and the *pgo* client, you will need the following -

 * a running Kubernetes or OpenShift cluster
 * the kubectl or oc clients installed in your PATH and configured to connect to the cluster (e.g. export KUBECONFIG=/etc/kubernetes/admin.conf)
 * a Kubernetes namespace created and set to where you want the operator installed. For this install we assume a namespace of *demo* has been created.
....
kubectl create -f examples/demo-namespace.json
kubectl config set-context $(kubectl config current-context) --namespace=demo
kubectl config view | grep namespace
....

[WARNING]
====
If you are not using the *demo* namespace, it will be required to edit the following and change the namespace where the service account and cluster role bindings will be deployed.

`$COROOT/deploy/service-account.yaml`

`$COROOT/deploy/cluster-role-binding.yaml`
====

Permissions are granted to the Operator by means of a Service Account called *postgres-operator*.  That service account is added to the Operator deployment.

The postgres-operator service account is granted cluster-admin priviledges using a cluster role binding *postgres-operator-cluster-role-binding*.

See link:https://kubernetes.io/docs/admin/authorization/rbac/[here] for more details on how to enable RBAC roles and modify the scope of the permissions to suit your needs.

== Basic Installation

The basic installation uses the default operator configuration settings, these settings assume you want to use HostPath storage on your Kube cluster for database persistence.  Other persistent options are available but require the Advanced Installation below.

=== Create HostPath Directory

The default Persistent Volume script assumes a default HostPath directory be created called */data*:
....
sudo mkdir /data
sudo chown 777 /data
....

Create some sample Persistent Volumes using the following script:
....
export CO_NAMESPACE=demo
export CO_CMD=kubectl
export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator
go get github.com/blang/expenv
$COROOT/pv/create-pv.sh
....

== Build Images & Deploy

{{%expand "Packaged Images" %}}

=== Packaged Images

To pull prebuilt versions from Dockerhub of the *postgres-operator* containers, specify the image versions, and execute the following Makefile target -
....
export CO_IMAGE_PREFIX=crunchydata
export CO_IMAGE_TAG=centos7-2.6
make pull
....

To pull down the prebuilt *pgo* binaries, download the *tar.gz* release file from the following link -

 * link:https://github.com/CrunchyData/postgres-operator/releases[Github Releases]
 * extract (e.g. tar xvzf postgres-operator.2.6-rc1.tar.gz)
....
cd $HOME
tar xvzf ./postgres-operator.2.6-rc1.tar.gz
....
 * copy *pgo* client to somewhere in your path (e.g. cp pgo /usr/local/bin)

Next, deploy the operator to your Kubernetes cluster -
....
cd $COROOT
make deployoperator
....

{{% /expand%}}

{{%expand "Build from Source" %}}

=== Build from Source

The purpose of this section is to illustrate how to build the PostgreSQL
Operator from source. These are considered advanced installation steps and
should be primarily used by developers or those wishing a more precise
installation method.

==== Requirements

The postgres-operator runs on any Kubernetes and Openshift platform that supports
Custom Resource Definitions. The Operator is tested on Kubeadm and OpenShift
Container Platform environments.

The operator is developed with the Golang versions greater than or equal to version 1.8. See
link:https://golang.org/dl/[Golang website] for details on installing golang.

The Operator project builds and operates with the following containers -

* link:https://hub.docker.com/r/crunchydata/pgo-lspvc/[PVC Listing Container]
* link:https://hub.docker.com/r/crunchydata/pgo-rmdata/[Remove Data Container]
* link:https://hub.docker.com/r/crunchydata/postgres-operator/[postgres-operator Container]
* link:https://hub.docker.com/r/crunchydata/pgo-apiserver/[apiserver Container]
* link:https://hub.docker.com/r/crunchydata/pgo-load/[file load Container]

This Operator is developed and tested on the following operating systems but is known to run on other operating systems -

* *CentOS 7*
* *RHEL 7*

First, install the project library dependencies. The godep dependency manager is used for this purpose. -
....
cd $COROOT
make setup
....

Then, compile the PostgreSQL Operator using the Makefile.
....
cd $COROOT
make all
which pgo
....

Finally, deploy the operator to your Kubernetes cluster.
....
cd $COROOT
make deployoperator
....

{{% /expand%}}

{{%expand "Makefile Targets" %}}

== Makefile Targets

The following table describes the Makefile targets:
.Makefile Targets
[width="80%",frame="topbot",options="header"]
|======================
|Target | Description
|all        | compile all binaries and build all images
|setup        | fetch the dependent packages required to build with
|deployoperator        | deploy the Operator (apiserver and postgers-operator) to Kubernetes
|main        | compile the postgres-operator
|runmain        | locally execute the postgres-operator
|pgo        | build the pgo binary
|runpgo        | run the pgo binary
|runapiserver        | run the apiserver binary outside of Kube
|clean        | remove binaries and compiled packages, restore dependencies
|operatorimage        | compile and build the postgres-operator Docker image
|apiserverimage        | compile and build the apiserver Docker image
|lsimage        | build the lspvc Docker image
|loadimage        | build the file load Docker image
|rmdataimage        | build the data deletion Docker image
|release        | build the postgres-operator release
|======================

{{% /expand%}}

{{%expand "Helm Chart" %}}

=== Helm Chart

First, pull prebuilt versions from Dockerhub of the *postgres-operator* containers,
specify the image versions, and execute the following Makefile target -
....
export CO_IMAGE_PREFIX=crunchydata
export CO_IMAGE_TAG=centos7-2.6
make pull
....

Then, build and deploy the operator using the provided Helm chart -
....
cd $COROOT/chart
helm install ./postgres-operator
helm ls
....

{{% /expand%}}

=== Verify Operator Status

To verify that the operator is deployed and running, run the following:
....
kubectl get pod --selector=name=postgres-operator
....

You should see output similar to this:
....
NAME                                 READY     STATUS    RESTARTS   AGE
postgres-operator-56598999cd-tbg4w   2/2       Running   0          1m
....

There are 2 containers in the operator pod, both should be *ready* as above.

When you first run the operator, it will create the required
CustomResourceDefinitions. You can view these as follows -
....
kubectl get crd
....

The operator creates the following Custom Resource Definitions over time as the
associated commands are triggered.
....
kubectl get crd
NAME                             AGE
pgbackups.cr.client-go.k8s.io    2d
pgclusters.cr.client-go.k8s.io   2d
pgingests.cr.client-go.k8s.io    2d
pgpolicies.cr.client-go.k8s.io   2d
pgreplicas.cr.client-go.k8s.io   2d
pgtasks.cr.client-go.k8s.io      2d
pgupgrades.cr.client-go.k8s.io   2d
....

At this point, the server side of the operator is deployed and ready.

The complete set of environment variables used in the installation
so far are -
....
export CO_IMAGE_PREFIX=crunchydata
export CO_IMAGE_TAG=centos7-2.6
export GOPATH=$HOME/odev
export GOBIN=$GOPATH/bin
export PATH=$PATH:$GOBIN
export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator
export CO_CMD=kubectl
....

You would normally add these into your *.bashrc* at this point to be used later on or if you want to redeploy the operator.

=== Configure *pgo* Client

The *pgo* command line client requires TLS for securing the connection to the operator's REST API.  This configuration is performed as follows:
....
export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key
....

The *pgo* client uses Basic Authentication to authenticate to the operator REST API, for authentication, add the following *.pgouser* file to your $HOME:
....
echo "username:password" > $HOME/.pgouser
....

The *pgo* client needs the URL to connect to the operator.

Depending on your Kube environment this can be done the following ways:

==== Running Kube Locally

If your local host is not set up to resolve Kube Service DNS names, you can specify the operator IP address as follows:
....
kubectl get service postgres-operator
NAME                TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
postgres-operator   NodePort   10.109.184.8   <none>        8443:30894/TCP   5m

export CO_APISERVER_URL=https://10.109.184.8:8443
pgo version
....

You can also define a bash alias like:
....
alias setip='export CO_APISERVER_URL=https://`kubectl get service postgres-operator -o=jsonpath="{.spec.clusterIP}"`:8443'
....

This alias will set the CO_APISERVER_URL IP address for you!

==== Running Kube Remotely

Set up a port-forward tunnel from your host to the Kube remote host, specifying the operator pod:
....
kubectl get pod --selector=name=postgres-operator
NAME                                 READY     STATUS    RESTARTS   AGE
postgres-operator-56598999cd-tbg4w   2/2       Running   0          8m

kubectl port-forward postgres-operator-56598999cd-tbg4w 8443:8443
....

In another terminal:
....
export CO_APISERVER_URL=https://127.0.0.1:8443
pgo version
....

=== Verify *pgo* Client

At this point you should be able to connect to the operator as follows:
....
pgo version
pgo client version 2.6
apiserver version 2.6
....

*pgo* commands are documented on the link:/getting-started/[Getting Started] page.

== Storage Configuration

Most users after they try out the operator will want to create a more customized installation and deployment of the operator using specific storage types.

The operator will work with HostPath, NFS, Dynamic, and GKE Storage.

=== NFS

To configure the operator to use NFS for storage, a sample *pgo.yaml.nfs* file is provided.  Overlay the default *pgo.yaml* file with that file:
....
cp $COROOT/examples/pgo.yaml.nfs $COROOT/conf/apiserver/pgo.yaml
....

Edit the *pgo.yaml* file to specify the NFS GID that is set for the NFS volume mount you will be using, the default value assumed is *nfsnobody* as the GID (65534).  Update the value to meet your NFS security settings.

There is currently no script available to create your NFS Persistent Volumes but you can typically modify the $COROOT/pv/create-pv.sh script to work with NFS.

=== Dynamic

To configure the operator to use Dynamic Storage classes for storage, a sample *pgo.yaml.storageclass* file is provided.  Overlay the default *pgo.yaml* file with that file:
....
cp $COROOT/examples/pgo.yaml.storageclass $COROOT/conf/apiserver/pgo.yaml
....

Edit the *pgo.yaml* file to specify the storage class you will be using, the default value assumed is *standard* which is the name used by default within a GKE Kube cluster deployment.  Update the value to match your storage classes.

Notice that the *FsGroup* setting is required for most block storage and is set to the value of *26* since the PostgreSQL container runs as UID *26*.

=== GKE

Some notes for setting up GKE for the Operator deployment.

==== Install Kubectl
On your host you will be working from, install the kubectl command:

https://kubernetes.io/docs/tasks/tools/install-kubectl/

==== GCP

* Select your project
* Create a Kube cluster in that project

By default a storage class called *standard* is created.

==== Install GCloud

To access the Kube cluster you need to install the gcloud utility:

....
https://cloud.google.com/sdk/downloads
cd google-cloud-sdk
./install.sh
....

==== Configure Kubectl for Cluster Access

....
gcloud auth login

gcloud container clusters get-credentials jeff-quickstart --zone us-central1-a --project crunchy-dev-test

kubectl get storageclass
....

{{% /expand%}}

{{%expand "I've installed the Operator. Now what?" %}}

== Next Steps

There are many ways to configure the operator further. Some sample configurations are
documented on the link:/configuration/[Configuration] page.

You may also want to find out more information on how the operator is designed to work and
deploy. This information can be found in the link:/how-it-works/[How It Works] page.

Information can be found on the full scope of commands on the
link:/getting-started/[Getting Started] page.

{{% /expand%}}
